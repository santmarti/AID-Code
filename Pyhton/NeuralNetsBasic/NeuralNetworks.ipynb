{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning simplest test: learn to multiply by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [0 1 2 3 4 5 6 7 8 9]\n",
      "target: [ 0  2  4  6  8 10 12 14 16 18]\n"
     ]
    }
   ],
   "source": [
    "x,y = np.arange(10), 2*np.arange(10)\n",
    "print(\"input:\",x)\n",
    "print(\"target:\", y)\n",
    "w = -3.0                 # initialize the weight \n",
    "alpha = 0.01             # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current w: 1.9518768524322019   Error: 0.14821358923725272\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(x))        # choose i.i.d training sample \n",
    "w += alpha*(y[i]-w*x[i])             # w = w + alpha*error  \n",
    "error = np.sum(np.power(x[i]*w - y[i],2))\n",
    "print(\"Current w:\", w, \"  Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Phase: Samples not in the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set [9.30579349 8.54358114 5.27636348 2.98318257 4.77010884]\n",
      "Error on test set:  0.5073626727991621\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(10,size=5)\n",
    "y = 2*x                                 # usually DataSet is divided between training set and test set\n",
    "print(\"Test Set\", x)\n",
    "error = np.sum(np.power(x*w-y,2))\n",
    "print(\"Error on test set: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning simplest test with biass: learn to multiply and sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [0 1 2 3 4 5 6 7 8 9]\n",
      "target: [ 1  3  5  7  9 11 13 15 17 19]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "y = 2*x+1\n",
    "print(\"input:\",x)\n",
    "print(\"target:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current w: 2.2628196240162732   Error: 6.032367947431148\n"
     ]
    }
   ],
   "source": [
    "# one weight cannot learn biass\n",
    "i = np.random.randint(len(x))     # choose i.i.d sample \n",
    "w += alpha*(y[i]-w*x[i])          # w = w + alpha*error    \n",
    "error = np.sum(np.power(x*w - y,2))\n",
    "print(\"Current w:\", w, \"  Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets consider two weights\n",
    "w = [0.1, 3.3]  # w0 = biass, w1 = the weight\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 w: [0.8734382847155945, 2.0188684172399727]   Error: 0.04672183840934185\n"
     ]
    }
   ],
   "source": [
    "def g(x,w):\n",
    "    return w[1]*x + w[0]\n",
    "\n",
    "gsteps, error = 0, 1000000\n",
    "while gsteps < 500 and error > 0.05:\n",
    "    i = np.random.randint(len(x))               # choose random training sample: must be i.i.d. \n",
    "    w[0] += alpha*(y[i]-g(x[i],w))      \n",
    "    w[1] += alpha*(y[i]-g(x[i],w))*x[i] \n",
    "    error = np.sum(np.power(g(x,w) - y,2))\n",
    "    print(gsteps, \"w:\", w, \"  Error:\", error)\n",
    "    gsteps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing one layer Neural Network in keras (Similar to Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_function():\n",
    "    \n",
    "    def __init__(self, inputs=2, outputs=1):\n",
    "        self.inputs, self.outputs = inputs, outputs\n",
    "        self.learning_rate = 0.001        \n",
    "        self.model = Sequential(name=\"PerceptronNetwork\")\n",
    "        self.model.add(Dense(outputs, input_shape=(inputs,), activation=\"linear\", kernel_initializer='random_uniform'))        \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=self.learning_rate))\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, s):\n",
    "        s = s.flatten()         # we can discard input dimension\n",
    "        s_batch = np.reshape(s, [1, s.shape[0]])\n",
    "        return self.model.predict(s_batch)[0][0]\n",
    "\n",
    "    def update(self, inputs, targets):\n",
    "        inputs = inputs.flatten()\n",
    "        inputs_batch = np.reshape(inputs, [1, inputs.shape[0]])\n",
    "        targets_batch = np.array([targets])\n",
    "        self.model.train_on_batch(inputs_batch, targets_batch)\n",
    "\n",
    "    def update_batch(self, inputs, targets):\n",
    "        self.model.train_on_batch(inputs, targets)\n",
    "        \n",
    "    def print_weights(self):\n",
    "        w = self.model.get_weights()\n",
    "        print(\"w:\",w[0].flatten(), \"b:\",w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn logic gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martisanchez/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"PerceptronNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "w: [-0.00557284 -0.04531506] b: [0.]\n"
     ]
    }
   ],
   "source": [
    "def gen_input(igate=0):\n",
    "    or_gate = {(0,0):0, (0,1):1, (1,0):1, (1,1):1}\n",
    "    and_gate = {(0,0):0, (0,1):0, (1,0):0, (1,1):1}\n",
    "    xor_gate = {(0,0):0, (0,1):1, (1,0):1, (1,1):0}\n",
    "    \n",
    "    if igate == 0: gate = or_gate\n",
    "    elif igate == 1: gate = and_gate\n",
    "    else: gate = xor_gate\n",
    "    \n",
    "    key = random.sample(list(gate.keys()),1)[0]\n",
    "    s = np.array(key) \n",
    "    y = gate[key]\n",
    "    return s,y\n",
    "\n",
    "f = Perceptron_function(inputs=2)\n",
    "f.print_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### And Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/martisanchez/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "w: [0.48819682 0.49553928] b: [-0.23419617]\n",
      "[1 0] 0 0.25400066\n",
      "[0 1] 0 0.26134312\n",
      "[1 0] 0 0.25400066\n",
      "[1 0] 0 0.25400066\n",
      "[0 0] 0 -0.23419617\n",
      "[1 0] 0 0.25400066\n",
      "[1 0] 0 0.25400066\n",
      "[1 0] 0 0.25400066\n",
      "[1 0] 0 0.25400066\n",
      "[1 0] 0 0.25400066\n",
      "MSE error = 0.6392787712717338\n"
     ]
    }
   ],
   "source": [
    "igate = 1 # OR Gate\n",
    "for _ in range(5000):          # train the network with input-output pairs\n",
    "    s,y = gen_input(igate)\n",
    "    f.update(s, y)\n",
    "    #s_batch, y_batch = np.array([s]), np.array([y])\n",
    "    #f.update_batch(s_batch, y_batch)\n",
    "\n",
    "f.print_weights()\n",
    "\n",
    "error = 0\n",
    "for _ in range(10):          # test the network with mean squared error (MSE)\n",
    "    s,y = gen_input(igate)\n",
    "    print(s,y,f.predict(s))\n",
    "    error += np.power(f.predict(s)-y,2)  # MSE\n",
    "    \n",
    "print(\"MSE error =\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_function():\n",
    "    \n",
    "    def __init__(self, inputs=4, outputs=1):\n",
    "        self.inputs, self.outputs = inputs, outputs\n",
    "        self.model = Sequential(name=\"NN\")\n",
    "        self.model.add(Dense(8, input_shape=(inputs,), activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(outputs, activation=\"linear\"))\n",
    "\n",
    "        self.learning_rate = 0.001        \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=self.learning_rate))\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, s):\n",
    "        s = s.flatten()         # we can discard input dimension\n",
    "        s_batch = np.reshape(s, [1, s.shape[0]])\n",
    "        return self.model.predict(s_batch)[0][0]\n",
    "\n",
    "    def update(self, inputs, targets):\n",
    "        inputs = inputs.flatten()\n",
    "        inputs_batch = np.reshape(inputs, [1, inputs.shape[0]])\n",
    "        targets_batch = np.array([targets])\n",
    "        self.model.train_on_batch(inputs_batch, targets_batch)\n",
    "    \n",
    "    def update_batch(self, inputs, targets):\n",
    "        self.model.train_on_batch(inputs, targets)\n",
    "        \n",
    "    def print_weights(self):\n",
    "        w = self.model.get_weights()\n",
    "        print(\"w:\",w[0].flatten(), \"b:\",w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learn logic gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Train using NN_function (not the perceptron) all logic gates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Neural Network to Sum \n",
    "$f(x=[...]) = \\sum_{i \\in x}{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Make a NN (use NN_function) learn to sum 4 input numbers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an LSTM Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_function():\n",
    "    \n",
    "    def __init__(self, inputs=4, outputs=1, steps=10):\n",
    "        self.inputs, self.outputs = inputs, outputs\n",
    "        self.model = Sequential(name=\"LSTM_network\")\n",
    "        self.model.add(LSTM(steps, activation='relu', input_shape=(steps, inputs), return_sequences=False, stateful=False))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(outputs, activation='linear'))\n",
    "        \n",
    "        self.learning_rate = 0.001        \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=self.learning_rate))\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, s):\n",
    "        s_batch = np.reshape(s, [1]+list(s.shape))\n",
    "        return self.model.predict(s_batch)[0][0]\n",
    "\n",
    "    def update(self, s, y):\n",
    "        s_batch = np.reshape(s, [1]+list(s.shape))\n",
    "        y_batch = np.reshape(np.array([y]), [1, self.outputs])\n",
    "        self.model.fit(s_batch, y_batch, verbose=0)\n",
    "\n",
    "    def update_batch(self, inputs, targets):\n",
    "        self.model.train_on_batch(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an LSTM Network to predict the sin function\n",
    "$f([sin(x_{t-4}+dx),...,sin(x_{t-1}+dx)])=sin(x_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_140 (Dense)            (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 15)                1020      \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 16)                256       \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,293\n",
      "Trainable params: 1,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "MSE errors (NN,LSTM) = 2.8220056337007664 1.924811977591568\n"
     ]
    }
   ],
   "source": [
    "def gen_input(nsteps):\n",
    "    x_ini = np.random.rand()\n",
    "    x_step = np.random.rand()\n",
    "    sin_seq = [np.sin(x_ini+i*x_step)  for i in range(nsteps+1)]\n",
    "    s = np.array(sin_seq[:-1]) \n",
    "    s = s[:, np.newaxis]         # array (N,) needs conversion to row vector\n",
    "    y = sin_seq[-1]  \n",
    "    return np.array(s),y\n",
    "\n",
    "nsteps = 15\n",
    "f = NN_function(inputs=nsteps, outputs=1)\n",
    "f_lstm = LSTM_function(inputs=1, outputs=1, steps=nsteps)\n",
    "\n",
    "n = 1000\n",
    "for _ in range(n):            # train the network with input-output pairs\n",
    "    s,y = gen_input(nsteps)\n",
    "    f.update(s, y)\n",
    "    f_lstm.update(s, y)\n",
    "\n",
    "error, error_lstm = 0, 0\n",
    "for _ in range(int(n/10)):         # test the network with mean squared error (MSE)\n",
    "    s,y = gen_input(nsteps)    \n",
    "    error += np.power(f.predict(s)-y,2)  # MSE\n",
    "    error_lstm += np.power(f_lstm.predict(s)-y,2)  # MSE\n",
    "    \n",
    "print(\"MSE errors (NN,LSTM) =\", error, error_lstm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
